import gymnasium as gym
import numpy as np
import argparse
import sys
from ..algorithms.sarsa import SarsaAgent
from ..algorithms.Qlr import QAgent
from ..algorithms.DQN import DQN
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='CliffWalking å¼ºåŒ–å­¦ä¹ è®­ç»ƒ')
    parser.add_argument('--algorithm', '-a', 
                       choices=['sarsa', 'qlearning', 'dqn'], 
                       default='sarsa',
                       help='é€‰æ‹©ç®—æ³•: sarsa, qlearning æˆ– dqn (é»˜è®¤: sarsa)')
    parser.add_argument('--episodes', '-e', 
                       type=int, 
                       default=2000,
                       help='è®­ç»ƒå›åˆæ•° (é»˜è®¤: 1000)')
    parser.add_argument('--epsilon', 
                       type=float, 
                       default=0.5,
                       help='æ¢ç´¢ç‡ (é»˜è®¤: 0.05)')
    parser.add_argument('--alpha', 
                       type=float, 
                       default=0.1,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.1)')
    parser.add_argument('--gamma', 
                       type=float, 
                       default=0.99,
                       help='æŠ˜æ‰£å› å­ (é»˜è®¤: 0.99)')

    args = parser.parse_args()
    
    # è®­ç»ƒæ—¶ä¸éœ€è¦æ¸²æŸ“ï¼Œé€Ÿåº¦æ›´å¿«
    env_train = gym.make('CliffWalking-v0')
    # æµ‹è¯•æ—¶ä½¿ç”¨å›¾å½¢æ¸²æŸ“
    env_test = gym.make('CliffWalking-v0', render_mode='human')


    # æ ¹æ®ç”¨æˆ·é€‰æ‹©åˆ›å»ºæ™ºèƒ½ä½“
    if args.algorithm == 'sarsa':
        print("ä½¿ç”¨ SARSA ç®—æ³•")
        MyAgent = SarsaAgent(
            obs_n=env_train.observation_space.n, 
            act_n=env_train.action_space.n,
            alpha=args.alpha,
            gamma=args.gamma,
            epsilon=args.epsilon,
        )
    elif args.algorithm == 'qlearning':
        print("âš¡ ä½¿ç”¨ Q-Learning ç®—æ³•")
        MyAgent = QAgent(
            obs_n=env_train.observation_space.n, 
            act_n=env_train.action_space.n,
            alpha=args.alpha,
            gamma=args.gamma,
            epsilon=args.epsilon
        )
    elif args.algorithm == 'dqn':
        print("ğŸ§  ä½¿ç”¨ DQN ç®—æ³•")
        MyAgent = DQN(
            obs_n=env_train.observation_space.n,  # ç»Ÿä¸€ä½¿ç”¨obs_n
            act_n=env_train.action_space.n,       # ç»Ÿä¸€ä½¿ç”¨act_n
            learning_rate=args.alpha,
            gamma=args.gamma,
            epsilon=args.epsilon
        )
    
    print("å¼€å§‹è®­ç»ƒ...")
    # ä½¿ç”¨æ— æ¸²æŸ“ç¯å¢ƒè¿›è¡Œå¿«é€Ÿè®­ç»ƒ
    if args.algorithm == 'dqn':
        episode_rewards = MyAgent.train(env_train, episode=args.episodes, epsilon=args.epsilon, gamma=args.gamma)
    else:
        episode_rewards = MyAgent.train(env_train, episodes=args.episodes, print_every=max(1, args.episodes//20))

    # ç»˜åˆ¶è®­ç»ƒå¥–åŠ±æ›²çº¿
    print("\nç”Ÿæˆè®­ç»ƒæ›²çº¿...")
    MyAgent.plot_reward(episode_rewards)
    
    # åˆ†æç»“æœ
    final_avg = np.mean(episode_rewards[-100:])
    print(f"\n=== è®­ç»ƒç»“æœ ===")
    print(f"ç®—æ³•: {args.algorithm.upper()}")
    print(f"æœ€å100å›åˆå¹³å‡å¥–åŠ±: {final_avg:.2f}")


    print("\nå¼€å§‹å›¾å½¢æ¼”ç¤º...")

    MyAgent.play_episode(env_test)

if __name__ == "__main__":
    main()
