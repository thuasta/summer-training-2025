# 配置环境

**作者：Doc.**  
**日期：2025.7.11**

## 安装conda和配系统环境变量

### 第一步 下载安装包

下载地址为：https://www.anaconda.com/

<img src="image.png" alt="alt text" width="400">
<img src="image-1.png" alt="alt text" width="400">

下载后安装：
<img src="image-2.png" alt="alt text" width="400">

### 第二步 把刚才安装好的conda配置到系统环境变量里面

#### 按下win键，搜索"编辑系统的环境变量"
<img src="image-3.png" alt="alt text" width="400">

点击环境变量：
<img src="image-4.png" alt="alt text" width="400">

点击path，进入后新建三个环境变量，注意修改路径为自己刚下载的anaconda路径！
<img src="image-6.png" alt="alt text" width="400">

### 第三步 检查是否成功安装

输入 `conda --version`

如果得到了：
<img src="image-7.png" alt="alt text" width="400">

则恭喜你，成功了！

## 使用conda创建一个虚拟环境，并在这个环境里装包

### 第一步 创建虚拟环境rl（你可以随意命名，可以把不是rl）

在任意打开一个终端
<img src="image-8.png" alt="alt text" width="400">

使用命令：
```bash
conda create -n rl python==3.10
```

这里为了在创建环境的时候指定了python解释器的版本，避免疏漏

激活环境：
```bash
conda activate rl
```

<img src="image-9.png" alt="alt text" width="400">

至此你可以看到左边出现了小括号，指示着我们当前所处的环境是rl

使用conda命令安装包管理器pip，用来安装其余必要的库：
<img src="image-10.png" alt="alt text" width="400">

安装所有必要的包：

首先你需要用一次conda额外安装：
```bash
conda install -c conda-forge box2d-py
```

否则你将不能玩月球登录游戏呜呜呜

这里你可以选择两种方法去做：

**方法一：**

在你的环境里面命令行输入：
```bash
pip install torch gymnasium matplotlib
```

（没错，可以一行全写完）

以及记得：
```bash
pip install "gymnasium[toy-text]"
```

否则你将不能可视化游戏过程呜呜

**方法二：**

在命令行中切换到本路径，输入：
```bash
pip install -r requirements.txt
```

（按照我给好的包依行安装）

**注意！！！！！！！！！** 库是在虚拟环境rl里面安装的，我们跑python代码所有的命令要在我们的环境里执行，否则没有解释器，也找不到相应的库

## 动手跑跑demo

注意，你需要先git clone（或者fork）：
```bash
git clone https://github.com/thuasta/summer-training-2025
```

在本部分中，为了避免路径的混淆，你需要始终保持在根路径下输入所有命令，即如下路径：
<img src="image-11.png" alt="alt text" width="400">

### 悬崖游走
```bash
python -m rl.runners.cliffwalking --algorithm dqn
```

最后一个参数可以改为sarsa, qlearning, dqn详情参照技术文档

### 摆锤平衡
```bash
python -m rl.runners.cartpole
```

或者
```bash
python ./rl/runners/cartpole.py
```

### 月球登录
```bash
python -m rl.runners.LunarLander
```

或者
```bash
python ./rl/runners/LunarLander.py
```

大家可以自行查阅一下这两个命令的区别

## 完成作业(选做)

策略梯度（Policy Gradient, PG）是强化学习中一种直接优化策略的方法，它面临一个显著问题：梯度估计的方差很大，导致训练不稳定、收敛缓慢。因此，研究并实现有效的方差减小技术，对于提升 PG 方法的实用性和效率具有重要意义。

**作业目标：**
- 理解策略梯度估计中的方差来源
- 任选一种经典的方差减小技术（baseline, GAE, reward normalization, advantage function 等）
- 将该技术应用于基本的策略梯度实现中
- 通过实验量化该方法对梯度估计方差的改善效果

## FAQ

日后强化到一定境界，出现了顿悟时刻，请务必带本文档作者发rl顶会论文ww